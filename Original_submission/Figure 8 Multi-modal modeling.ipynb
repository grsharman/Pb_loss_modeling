{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "close-arbor",
   "metadata": {},
   "source": [
    "#### Synthetic example where input U-Pb dates are derived from multiple Gaussian distributions. Mixture modeling is required to estimate the original Gaussian distributions prior to convolution with the Pb loss function.\n",
    "By: Glenn R. Sharman, Department of Geosciences, Universit of Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import convFuncs as convFunc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import convolve\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kstest\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from astropy.stats import kuiper\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import xlsxwriter\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # For improving matplotlib figure resolution\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42 # For allowing preservation of fonts upon importing into Adobe Illustrator\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gaussian distributions\n",
    "ages = [50, 52, 55]  # Mean ages of the normal distribution, in Ma\n",
    "omega_Mas = [1, 0.8, 1.2] # Standard deviations of the normal distribution, in Myr\n",
    "\n",
    "ns = [500, 350, 150] # Number of analyses to draw\n",
    "weightings = ns/np.sum(ns) # Proportion of analyses contributed by each Gaussian distribution\n",
    "\n",
    "n_x = 20001 # Number of x-axis values for both Ma and pct space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x-axis in % from age\n",
    "x1 = -100 # Note, it is not possible for a U-Pb date to be < -100% from it's true age, as this would result in a negative age\n",
    "x2 = 100\n",
    "x = np.linspace(x1, x2, n_x)\n",
    "\n",
    "# Calculate Gaussian pdfs and draw analyses from each\n",
    "norm_Ma_n = []\n",
    "norm_pct_n = []\n",
    "norm_Ma_pdfs = []\n",
    "xages = []\n",
    "for i in range(len(ages)):\n",
    "    # Define x-axis in Ma\n",
    "    xage_1 = 0 # Lower limit in Ma\n",
    "    xage_2 = ages[i]*2 # Upper limit in Ma\n",
    "    xage = np.linspace(xage_1, xage_2, n_x)\n",
    "    xages.append(xage)\n",
    "    \n",
    "    # Define the Gaussian distribution pdf and sample it randomly\n",
    "    rv_norm_Ma = norm(loc = ages[i], scale = omega_Mas[i])\n",
    "    norm_Ma_pdf = rv_norm_Ma.pdf(xage)\n",
    "    norm_Ma_pdfs.append(norm_Ma_pdf/np.sum(norm_Ma_pdf)) # Normalize so area under the curve = 1\n",
    "    norm_Ma_n.append(rv_norm_Ma.rvs(size=ns[i])) # Draw randomly from the normal distribution\n",
    "    norm_pct_n.append((norm_Ma_n[i]-ages[i])/ages[i]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because x-axis arrays are all different, must convert them to the same x-axis values\n",
    "xage_comb = np.linspace(0, np.min(ages)*2, n_x) # Use the minimum value of ages, so we don't get an error. Note, this will not work if the peaks are widely spaced from each other\n",
    "\n",
    "norm_Ma_pdfs_comb = []\n",
    "for i in range(len(ages)):\n",
    "    pdf_interp = interp1d(xages[i], norm_Ma_pdfs[i])\n",
    "    norm_Ma_pdfs_comb.append(pdf_interp(xage_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine pdfs into one weighted distribution\n",
    "\n",
    "norm_Ma_pdf_comb = np.zeros_like(norm_Ma_pdfs_comb[0])\n",
    "for i in range(len(ages)):\n",
    "    norm_Ma_pdf_comb += norm_Ma_pdfs_comb[i]*weightings[i]\n",
    "norm_Ma_pdf_comb = norm_Ma_pdf_comb/np.sum(norm_Ma_pdf_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot that compares the sampled data versus the overall probability distribution\n",
    "plt.plot(xage_comb, norm_Ma_pdf_comb, color='black')\n",
    "plt.ylim(0,)\n",
    "plt.twinx()\n",
    "plt.hist(norm_Ma_n, bins=25, histtype='bar', stacked=False, rwidth=1/len(ages)*100, alpha=1);\n",
    "\n",
    "plt.xlim(40,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-settlement",
   "metadata": {},
   "source": [
    "#### Create a Pb loss distribution and convolve it with each of the sub-distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(convFunc);\n",
    "\n",
    "dist_type_original = 'weibull'\n",
    "params = [5, 1.5] # Scale, shape\n",
    "\n",
    "Pb_loss_pct_pdf = convFunc.Pb_loss_fun(params, dist_type_original, x)\n",
    "\n",
    "conv_Ma_pdfs = []\n",
    "for i in range(len(ages)):\n",
    "    conv_Ma_pdfs.append(convolve(Pb_loss_pct_pdf, norm_Ma_pdfs_comb[i], mode='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale each convolved pdf by its associated weighting\n",
    "conv_Ma_pdf_comb = np.zeros_like(conv_Ma_pdfs[0])\n",
    "for i in range(len(ages)):\n",
    "    conv_Ma_pdf_comb += conv_Ma_pdfs[i]*weightings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace slightly negative values with 0\n",
    "conv_Ma_pdf_comb[conv_Ma_pdf_comb<0] = 0\n",
    "conv_Ma_pdf_comb = conv_Ma_pdf_comb/np.sum(conv_Ma_pdf_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dates from the convolved distribution\n",
    "dates_nonCA = np.random.choice(xage_comb, size=np.sum(ns), replace=True, p=conv_Ma_pdf_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Pb loss perturbed vs unperturbed U-Pb date distributions\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(xage_comb, norm_Ma_pdf_comb, color='navy')\n",
    "plt.ylim(0,)\n",
    "plt.twinx()\n",
    "plt.plot(xage_comb, conv_Ma_pdf_comb, color='red')\n",
    "plt.ylim(0,)\n",
    "plt.xlim(35,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Pb loss distribution\n",
    "fig, ax = plt.subplots(1, figsize=(3,3))\n",
    "\n",
    "ax.plot(x, Pb_loss_pct_pdf)\n",
    "ax.set_xlim(-20,0)\n",
    "ax.set_ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-albany",
   "metadata": {},
   "source": [
    "##### Conduct Gaussian mixture modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3)\n",
    "X = np.concatenate(norm_Ma_n).ravel() # Flatten the list of arrays\n",
    "gmm.fit(X.reshape(-1,1))\n",
    "\n",
    "#gmm_std_devs = np.sqrt(gmm.covariances_)\n",
    "gmm_std_devs = [np.float(x) for x in gmm.covariances_]\n",
    "gmm_means = [np.float(x) for x in gmm.means_]\n",
    "gmm_weights = [np.float(x) for x in gmm.weights_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_Ma_pdf = np.zeros_like(conv_Ma_pdfs[0])\n",
    "gmm_Ma_pdf_conv = np.zeros_like(conv_Ma_pdfs[0])\n",
    "for i in range(len(ages)):   \n",
    "    # Define the Gaussian distribution pdf and sample it randomly\n",
    "    rv_norm_Ma = norm(loc = gmm_means[i], scale = gmm_std_devs[i])\n",
    "    norm_Ma_pdf = rv_norm_Ma.pdf(xage_comb)\n",
    "    norm_Ma_pdf = norm_Ma_pdf/np.sum(norm_Ma_pdf) # Normalize so area under the curve = 1\n",
    "    gmm_Ma_pdf += norm_Ma_pdf*gmm_weights[i]\n",
    "    gmm_conv = convolve(Pb_loss_pct_pdf, norm_Ma_pdfs_comb[i], mode='same')\n",
    "    gmm_Ma_pdf_conv += gmm_conv*gmm_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot that compares the actual pdf vs modeled pdf\n",
    "plt.plot(xage_comb, conv_Ma_pdf_comb, '-', color='red', label='Pb loss perturbed')\n",
    "plt.plot(xage_comb, gmm_Ma_pdf, '--', color='navy', label='Modeled (no Pb loss)')\n",
    "plt.plot(xage_comb, norm_Ma_pdf_comb, '-', color='navy', label='Actual (no Pb loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(0,)\n",
    "plt.twinx()\n",
    "plt.hist(np.concatenate(norm_Ma_n).ravel(), bins=50, alpha=0.5, color='gray');\n",
    "plt.xlim(40, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-sound",
   "metadata": {},
   "source": [
    "#### Final step is to model Pb loss for all three Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(convFunc);\n",
    "\n",
    "dist_types = ['none','constant','isolated','uniform','gamma','expon','rayleigh','weibull','pareto','halfnorm','lognorm'] # Select which form(s) of Pb loss you want to model\n",
    "\n",
    "method = 'ss' # 'ss' is sum of squared residuals between ECDF and modeled CDF\n",
    "\n",
    "label = 'TestMultiModalMixture'\n",
    "\n",
    "xlim = (40, 60)\n",
    "xlim_Pb_loss = (-20, 0)\n",
    "dates_CA = np.concatenate(norm_Ma_n).ravel()\n",
    "\n",
    "plot_ref_age = False\n",
    "\n",
    "errors_nonCA = np.zeros_like(dates_nonCA)\n",
    "errors_CA = np.zeros_like(dates_CA)\n",
    "\n",
    "pathlib.Path(str(label)).mkdir(parents=True, exist_ok=True) # Recursively creates the directory and does not raise an exception if the directory already exists \n",
    "\n",
    "file_name = str(label)+'/'+'model_results_'+label+'.xlsx'\n",
    "\n",
    "plot_fig = True\n",
    "\n",
    "workbook = xlsxwriter.Workbook(file_name)\n",
    "\n",
    "bold_format = workbook.add_format({'bold' : True})\n",
    "\n",
    "max_offset = (np.max(ages)-np.min(dates_nonCA))/np.max(ages)*-100\n",
    "\n",
    "# Record model parameters\n",
    "worksheet = workbook.add_worksheet('Model_parameters')\n",
    "worksheet.write(0, 0, 'Sample', bold_format)\n",
    "worksheet.write(1, 0, 'N (non-CA)', bold_format)\n",
    "worksheet.write(0, 1, label)\n",
    "worksheet.write(1, 1, len(dates_nonCA))\n",
    "if dates_CA is not None:\n",
    "    worksheet.write(2, 0, 'N (CA)', bold_format)\n",
    "    worksheet.write(2, 1, len(dates_CA))\n",
    "    worksheet.write(3, 0, 'Misfit function', bold_format)\n",
    "    worksheet.write(3, 1, method)\n",
    "else:\n",
    "    worksheet.write(2, 0, 'Misfit function', bold_format)\n",
    "    worksheet.write(2, 1, method)\n",
    "                \n",
    "c = 0 # Counter variable\n",
    "worksheet = workbook.add_worksheet('Model_results')\n",
    "worksheet.write(0, 1, 'fun', bold_format)\n",
    "worksheet.write(0, 2, 'KS Dmax (f*g)', bold_format)\n",
    "worksheet.write(0, 3, 'KS p-value (f*g)', bold_format)\n",
    "worksheet.write(0, 4, 'Kuiper Vmax (f*g)', bold_format)\n",
    "worksheet.write(0, 5, 'Kuiper p-value (f*g)', bold_format)\n",
    "if dates_CA is not None:\n",
    "    worksheet.write(0, 6, 'KS Dmax (f)', bold_format)\n",
    "    worksheet.write(0, 7, 'KS p-value (f)', bold_format)\n",
    "    worksheet.write(0, 8, 'Kuiper Vmax (f)', bold_format)\n",
    "    worksheet.write(0, 9, 'Kuiper p-value (f)', bold_format)\n",
    "    worksheet.write(0, 10, 'f(t) age', bold_format)\n",
    "    c += 5\n",
    "worksheet.write(0, 6+c, 'f(t) 1 s.d.', bold_format)\n",
    "worksheet.write(0, 7+c, 'g(t) params[0]', bold_format)\n",
    "worksheet.write(0, 8+c, 'g(t) params[1]', bold_format)\n",
    "worksheet.write(0, 9+c, 'g(t) params[2]', bold_format)\n",
    "\n",
    "c = 0 # counter variable\n",
    "for dist_type in dist_types:\n",
    "    print('Starting ',dist_type)\n",
    "    if dist_type == 'none':\n",
    "        params_0 = [0] # Age (Ma), omega (Myr), and shift in %\n",
    "        bounds = [(0,0)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'constant':\n",
    "        params_0 = [-2.0] # Age (Ma), omega (Myr), and shift in %\n",
    "        bounds = [(max_offset,0)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'isolated':\n",
    "        params_0 = [-3, 0.8] # Age (Ma), omega (Myr), and shift in %, and proportion of grains with shift (0-1)\n",
    "        bounds = [(max_offset,0), (0,1)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'expon':\n",
    "        params_0 = [1.0] # Age (Ma), omega (Myr), and shape\n",
    "        bounds = [(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "        \n",
    "    if dist_type == 'rayleigh':\n",
    "        params_0 = [1.0] # Age (Ma), omega (Myr), and shape\n",
    "        bounds = [(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'halfnorm':\n",
    "        params_0 = [1.0] # Age (Ma), omega (Myr), and shape\n",
    "        bounds = [(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'lognorm':\n",
    "        params_0 = [1.0, 1.0] # Age (Ma), omega (Myr), shape, and scale\n",
    "        bounds = [(0,None), (0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "        \n",
    "    if dist_type == 'weibull':\n",
    "        params_0 = [1.0, 1.0] # scale, and shape\n",
    "        bounds = [(0,None),(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'gamma':\n",
    "        params_0 = [0.5, 1.0] # Age (Ma), omega (Myr), scale, and shape\n",
    "        bounds = [(0,None),(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'gengamma':\n",
    "        params_0 = [1.0, 1.0, 1.0] # Age (Ma), omega (Myr), scale, and shape\n",
    "        bounds = [(0,None),(0,None),(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'uniform':\n",
    "        params_0 = [1.0, 1.0] # Age (Ma), omega (Myr), u_min, and u_max\n",
    "        bounds = [(0,None),(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    if dist_type == 'pareto':\n",
    "        params_0 = [1] # Age (Ma), omega (Myr), shape\n",
    "        bounds = [(0,None)]\n",
    "        result = minimize(convFunc.misfit_poly, params_0, args=(dist_type, gmm_Ma_pdf, x, xage_comb, dates_nonCA, method), \n",
    "                      bounds=bounds, tol=1e-20, method='Powell', options={'maxiter' : 1e6, 'disp' : False})\n",
    "\n",
    "    Pb_loss_pct_pdf = convFunc.Pb_loss_fun(params=result.x, dist_type=dist_type, x=x)    \n",
    "    \n",
    "    norm_Ma_pdf_comb_fit = np.zeros_like(norm_Ma_pdfs_comb[0])\n",
    "    for i in range(len(ages)):\n",
    "        norm_Ma_pdf_comb_fit += norm_Ma_pdfs_comb[i]*gmm_weights[i]\n",
    "    norm_Ma_pdf_comb_fit = norm_Ma_pdf_comb_fit/np.sum(norm_Ma_pdf_comb_fit)\n",
    "\n",
    "    conv_Ma_pdf_fit = convolve(Pb_loss_pct_pdf, norm_Ma_pdf_comb_fit, mode='same')\n",
    "    \n",
    "    Pb_loss_pct_pdf_fit = convFunc.Pb_loss_fun(result.x, dist_type, x)\n",
    "\n",
    "    conv_pdf_fit = convolve(Pb_loss_pct_pdf_fit, gmm_Ma_pdf, mode='same')\n",
    "\n",
    "    ks_results = kstest(rvs=dates_nonCA, cdf=convFunc.cdf_fun(xage_comb, conv_pdf_fit))\n",
    "    kuiper_results = kuiper(data=dates_nonCA, cdf=convFunc.cdf_fun(xage_comb, conv_pdf_fit))\n",
    "    \n",
    "    if dates_CA is not None:\n",
    "        ks_results_f = kstest(rvs=dates_CA, cdf=convFunc.cdf_fun(xage_comb, norm_Ma_pdf_comb_fit))\n",
    "        kuiper_results_f = kuiper(data=dates_CA, cdf=convFunc.cdf_fun(xage_comb, norm_Ma_pdf_comb_fit))\n",
    "    \n",
    "    d = 0 # Counter variable\n",
    "    worksheet.write(c+1, 0, dist_type, bold_format)\n",
    "    worksheet.write(c+1, 1, result.fun)\n",
    "    worksheet.write(c+1, 2, ks_results[0])\n",
    "    worksheet.write(c+1, 3, ks_results[1])\n",
    "    worksheet.write(c+1, 4, kuiper_results[0])\n",
    "    worksheet.write(c+1, 5, kuiper_results[1])\n",
    "    if dates_CA is not None:\n",
    "        worksheet.write(c+1, 6, ks_results_f[0])\n",
    "        worksheet.write(c+1, 7, ks_results_f[1])\n",
    "        worksheet.write(c+1, 8, kuiper_results_f[0])\n",
    "        worksheet.write(c+1, 9, kuiper_results_f[1])\n",
    "        d += 5\n",
    "    for i in range(len(result.x)):\n",
    "        worksheet.write(c+1, 7+d+i, result.x[i])\n",
    "       \n",
    "    print('---{}: '.format(method), np.round(result.fun,6))\n",
    "    \n",
    "    for i in range(len(result.x)-2):\n",
    "        print('---g(t) params[{}]'.format(i),np.round(result.x[i+2],2))\n",
    "    c+=1\n",
    "    \n",
    "    if plot_fig:\n",
    "        fig = convFunc.plot_Pb_loss_model_poly(norm_Ma_pdf = norm_Ma_pdf_comb, conv_Ma_pdf=conv_pdf_fit, params_Pb_loss=result.x,\n",
    "                                                     fit=result.fun, dates_input=dates_nonCA, errors_1s_input=errors_nonCA, \n",
    "                                xage=xage_comb, x=x, xlim=xlim, xlim_Pb_loss=xlim_Pb_loss, dist_type=dist_type,\n",
    "                                plot_ref_age=plot_ref_age, ref_age=None, ref_age_2s_uncert=None, dates_input_CA=dates_CA,\n",
    "                                                    errors_1s_input_CA=errors_CA);\n",
    "        fig.savefig(str(label)+'/'+'fig_'+str(dist_type)+'.pdf')\n",
    "    \n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure that compares the true pdfs versus the modeled pdfs\n",
    "plt.plot(xage_comb, conv_pdf_fit, '--', label='Modeled', color='red')\n",
    "plt.plot(xage_comb, conv_Ma_pdf_comb, '-', label='Actual', color='red')\n",
    "\n",
    "plt.plot(xage_comb, gmm_Ma_pdf, '--', color='navy', label='Modeled')\n",
    "plt.plot(xage_comb, norm_Ma_pdf_comb, '-', color='navy', label='Actual')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(40,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a summary figure that illustrates the process of multi-modal modeling\n",
    "\n",
    "bins = np.linspace(40,60,50)\n",
    "colors = ['lightgray','slategray','black']\n",
    "xlim = [40,65]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5,10))\n",
    "\n",
    "Pb_loss_pct_pdf = convFunc.Pb_loss_fun(params, dist_type_original, x)\n",
    "\n",
    "for i in range(len(ages)):\n",
    "    axs[0].plot(xage_comb, norm_Ma_pdfs_comb[i]*weightings[i], color='black', alpha=1)\n",
    "    axs[0].fill_between(xage_comb, norm_Ma_pdfs_comb[i]*weightings[i], y2=0, color=colors[i], alpha=0.5)\n",
    "    axs[0].plot(xage_comb, norm_Ma_pdf_comb, color='navy')\n",
    "    axs[0].set_xlim(xlim[0],xlim[1])\n",
    "    axs[0].set_ylim(0,)\n",
    "    \n",
    "for i in range(len(ages)):\n",
    "    axs[1].plot(xage_comb, conv_Ma_pdfs[i]*weightings[i], color='black', alpha=1)\n",
    "    axs[1].fill_between(xage_comb, conv_Ma_pdfs[i]*weightings[i], y2=0, color=colors[i], alpha=0.5)\n",
    "    axs[1].plot(xage_comb, conv_Ma_pdf_comb, color='red')\n",
    "    axs[1].set_xlim(xlim[0],xlim[1])\n",
    "    axs[1].set_ylim(0,)\n",
    "\n",
    "axs[2].plot(xage_comb, norm_Ma_pdf_comb, color='navy')\n",
    "axs[2].plot(xage_comb, gmm_Ma_pdf, '--', color='navy', label='Modeled')\n",
    "\n",
    "axs[2].plot(xage_comb, conv_Ma_pdf_comb, color='red')\n",
    "axs[2].plot(xage_comb, conv_pdf_fit, '--', label='Modeled', color='red')\n",
    "\n",
    "axs[2].set_ylim(0,)\n",
    "#plt.twinx()\n",
    "#plt.hist(norm_Ma_n, bins=bins, histtype='bar', stacked=False, rwidth=1/len(ages)*100, color=['lightgray','slategray','black']);\n",
    "axs[2].set_xlim(xlim[0],xlim[1])\n",
    "\n",
    "# Create a smaller subplot to show the distribution of Pb loss in the sample\n",
    "ax_sub = axs[2].inset_axes([0.65, 0.5, 0.3, 0.4], transform=axs[2].transAxes)\n",
    "\n",
    "ax_sub.plot(x, np.cumsum(Pb_loss_pct_pdf), color='black')\n",
    "ax_sub.plot(x, np.cumsum(Pb_loss_pct_pdf_fit), '--', color='black')\n",
    "#ax_sub.axvline(x=0.0, ymin=0, ymax=1, ls='--', color='gray')\n",
    "#ax_sub.set_title('''Cumulative\n",
    "#apparent Pb loss''', ha='center')\n",
    "\n",
    "# Make the y-axis scale to the 99th percentile (to avoid it compressing when gamma values approach infinity)\n",
    "#p95_ind = next(i for i, v in enumerate(np.cumsum(Pb_loss_pct_pdf)) if v > 0.95)\n",
    "#p05_ind = next(i for i, v in enumerate(np.cumsum(Pb_loss_pct_pdf)) if v > 0.05)\n",
    "#ax_sub.set_xlim(x[p05_ind], 0)\n",
    "#ax_sub.set_ylim(0,Pb_loss_pct_pdf[p95_ind])\n",
    "\n",
    "ax_sub.set_xlim(xlim_Pb_loss[0],xlim_Pb_loss[1])\n",
    "ax_sub.set_ylim(0,1)\n",
    "ax_sub.set_xlabel('Age offset (%)')\n",
    "\n",
    "axs[2].set_xlabel('Age (Ma)')\n",
    "\n",
    "#plt.subplots_adjust(hspace=0)\n",
    "\n",
    "#plt.plot(xage_comb, conv_Ma_pdf_comb)\n",
    "#plt.plot(xage_comb, norm_Ma_pdf_comb)\n",
    "plt.xlim(xlim[0],xlim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-empire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
